{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392e02be",
   "metadata": {},
   "source": [
    "# Simple RAG Pipeline for STEM OPT Document\n",
    "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) workflow using LangChain, FAISS, and LLMs to answer questions about the STEM OPT extension process. Each section is annotated for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e238138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf5b17",
   "metadata": {},
   "source": [
    "## Load and Parse the STEM OPT HTML Document\n",
    "Read the USCIS STEM OPT extension HTML file and extract its content using BeautifulSoup for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data/documents/Optional Practical Training Extension for STEM Students (STEM OPT) _ USCIS.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Parse HTML and extract text\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84daab46",
   "metadata": {},
   "source": [
    "### Extract Relevant Content\n",
    "Identify and extract the main content panels from the HTML using their CSS class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "panels = soup.find_all(class_=\"accordion__panel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [panel.get_text(separator=\"\\n\", strip=True) for panel in panels]\n",
    "combined_text = \"\\n\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8f735",
   "metadata": {},
   "source": [
    "### Chunk the Extracted Text\n",
    "Split the combined text into smaller chunks to fit within the token limits of embedding models and LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=500, separator=\"\\n\\n\"):\n",
    "    \n",
    "    # Split by paragraphs (double newlines)\n",
    "    paragraphs = text.split(separator)\n",
    "    print(paragraphs)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) < max_tokens:\n",
    "            current_chunk += para + separator\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = para + separator\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307589e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(combined_text, max_tokens=200)  # adjust size as needed\n",
    "print(f\"Created {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb66f0",
   "metadata": {},
   "source": [
    "### Create Document Objects\n",
    "Convert each text chunk into a LangChain `Document` object for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2040f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1ac83",
   "metadata": {},
   "source": [
    "### Embed Documents and Build Vector Store\n",
    "Generate embeddings for each document chunk and store them in a FAISS vector database for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adeae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fc378",
   "metadata": {},
   "source": [
    "### Retrieve Relevant Chunks for a Query\n",
    "Set up a retriever from the vector store and use it to fetch the most relevant document chunks for a sample user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd02a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "query = \"what's the process for applying OPT?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233947f8",
   "metadata": {},
   "source": [
    "### Prepare Retrieved Content for LLM\n",
    "Combine the retrieved document chunks into a single context string to be used as input for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_docs_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b9a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_docs_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8650aa",
   "metadata": {},
   "source": [
    "### Construct the Prompt for the LLM\n",
    "Create a prompt template that provides the retrieved context and user query to the language model for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb844a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert assistant. Use the context below to answer the user's question.\n",
    "Do NOT include any internal thoughts or explanations.\n",
    "\n",
    "Context:\n",
    "{documents}\n",
    "\n",
    "User Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2db422",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"documents\", \"query\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaef017",
   "metadata": {},
   "source": [
    "### Set Up LLMs and Chains\n",
    "Instantiate both a local (Ollama) and an online (Together API) language model, and set up LLM chains for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3 = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c031b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '34f5f526391626c1e46bb060671b85eaf8ec355a22fdb8292dc147fe6d4b3df7'\n",
    "\n",
    "online_llm = ChatOpenAI(\n",
    "    model=\"meta-llama/Llama-Vision-Free\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://api.together.xyz/v1\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cea21d",
   "metadata": {},
   "source": [
    "### Generate Answers Using LLMs\n",
    "Run both the online and local LLM chains to generate answers to the user query based on the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f18861",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_llm_chain = LLMChain(llm=online_llm, prompt=prompt)\n",
    "\n",
    "# this took 2 s to generate summary\n",
    "online_llm_summary = online_llm_chain.run(documents=combined_docs_text, query=query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_chain = LLMChain(llm=llama3, prompt=prompt)\n",
    "\n",
    "#local model-  this took 7m to generate summary\n",
    "llama3_summary = llama3_chain.run(documents=combined_docs_text, query=query) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c912f",
   "metadata": {},
   "source": [
    "### Output and Compare Results\n",
    "Display the answers generated by both the online and local LLMs for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(online_llm_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1716328",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llama3_summary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
